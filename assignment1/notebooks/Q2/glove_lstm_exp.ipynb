{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchtext.vocab import GloVe\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%¯¸\n",
    "df_train = pd.read_csv(\"../../data/Text-To-SQL-COL775/train.csv\")\n",
    "df_tables = pd.read_json(\"../../data/Text-To-SQL-COL775/tables.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column_names</th>\n",
       "      <th>column_names_original</th>\n",
       "      <th>column_types</th>\n",
       "      <th>db_id</th>\n",
       "      <th>foreign_keys</th>\n",
       "      <th>primary_keys</th>\n",
       "      <th>table_names</th>\n",
       "      <th>table_names_original</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[-1, *], [0, perpetrator id], [0, people id],...</td>\n",
       "      <td>[[-1, *], [0, Perpetrator_ID], [0, People_ID],...</td>\n",
       "      <td>[text, number, number, text, number, text, tex...</td>\n",
       "      <td>perpetrator</td>\n",
       "      <td>[[2, 9]]</td>\n",
       "      <td>[1, 9]</td>\n",
       "      <td>[perpetrator, people]</td>\n",
       "      <td>[perpetrator, people]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[-1, *], [0, building], [0, room number], [0,...</td>\n",
       "      <td>[[-1, *], [0, building], [0, room_number], [0,...</td>\n",
       "      <td>[text, text, text, number, text, text, number,...</td>\n",
       "      <td>college_2</td>\n",
       "      <td>[[9, 4], [13, 4], [19, 1], [20, 2], [15, 7], [...</td>\n",
       "      <td>[1, 4, 7, 11, 15, 22, 27, 31, 37, 39, 45]</td>\n",
       "      <td>[classroom, department, course, instructor, se...</td>\n",
       "      <td>[classroom, department, course, instructor, se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[-1, *], [0, id], [0, city], [0, country], [0...</td>\n",
       "      <td>[[-1, *], [0, id], [0, City], [0, Country], [0...</td>\n",
       "      <td>[text, number, text, text, text, text, text, n...</td>\n",
       "      <td>flight_company</td>\n",
       "      <td>[[20, 7], [19, 1]]</td>\n",
       "      <td>[1, 7, 13]</td>\n",
       "      <td>[airport, operate company, flight]</td>\n",
       "      <td>[airport, operate_company, flight]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[-1, *], [0, institution id], [0, name], [0, ...</td>\n",
       "      <td>[[-1, *], [0, instID], [0, name], [0, country]...</td>\n",
       "      <td>[text, number, text, text, number, text, text,...</td>\n",
       "      <td>icfp_1</td>\n",
       "      <td>[[11, 7], [10, 1], [9, 4]]</td>\n",
       "      <td>[1, 4, 7, 9]</td>\n",
       "      <td>[institution, authors, papers, authorship count]</td>\n",
       "      <td>[Inst, Authors, Papers, Authorship]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[-1, *], [0, body builder id], [0, people id]...</td>\n",
       "      <td>[[-1, *], [0, Body_Builder_ID], [0, People_ID]...</td>\n",
       "      <td>[text, number, number, number, number, number,...</td>\n",
       "      <td>body_builder</td>\n",
       "      <td>[[2, 6]]</td>\n",
       "      <td>[1, 6]</td>\n",
       "      <td>[body builder, people]</td>\n",
       "      <td>[body_builder, people]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>[[-1, *], [0, employee ssn], [0, project numbe...</td>\n",
       "      <td>[[-1, *], [0, Essn], [0, Pno], [0, Hours], [1,...</td>\n",
       "      <td>[text, number, number, number, text, text, tex...</td>\n",
       "      <td>company_1</td>\n",
       "      <td>[]</td>\n",
       "      <td>[1, 7, 15, 19, 22, 27]</td>\n",
       "      <td>[works on, employee, department, project, depe...</td>\n",
       "      <td>[works_on, employee, department, project, depe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>[[-1, *], [0, workshop id], [0, date], [0, ven...</td>\n",
       "      <td>[[-1, *], [0, Workshop_ID], [0, Date], [0, Ven...</td>\n",
       "      <td>[text, number, text, text, text, number, numbe...</td>\n",
       "      <td>workshop_paper</td>\n",
       "      <td>[[10, 1], [9, 5]]</td>\n",
       "      <td>[1, 5, 9]</td>\n",
       "      <td>[workshop, submission, acceptance]</td>\n",
       "      <td>[workshop, submission, Acceptance]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>[[-1, *], [0, item id], [0, title], [1, a id],...</td>\n",
       "      <td>[[-1, *], [0, i_id], [0, title], [1, a_id], [1...</td>\n",
       "      <td>[text, number, text, number, number, number, n...</td>\n",
       "      <td>epinions_1</td>\n",
       "      <td>[[5, 1], [4, 8], [11, 8], [10, 8]]</td>\n",
       "      <td>[1, 3, 8]</td>\n",
       "      <td>[item, review, useracct, trust]</td>\n",
       "      <td>[item, review, useracct, trust]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>[[-1, *], [0, party id], [0, party theme], [0,...</td>\n",
       "      <td>[[-1, *], [0, Party_ID], [0, Party_Theme], [0,...</td>\n",
       "      <td>[text, number, text, text, text, text, number,...</td>\n",
       "      <td>party_host</td>\n",
       "      <td>[[11, 1], [12, 7]]</td>\n",
       "      <td>[1, 7, 11]</td>\n",
       "      <td>[party, host, party host]</td>\n",
       "      <td>[party, host, party_host]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>[[-1, *], [0, attribute id], [0, attribute nam...</td>\n",
       "      <td>[[-1, *], [0, attribute_id], [0, attribute_nam...</td>\n",
       "      <td>[text, number, text, text, number, text, text,...</td>\n",
       "      <td>product_catalog</td>\n",
       "      <td>[[10, 4], [13, 9], [27, 9], [26, 12]]</td>\n",
       "      <td>[1, 4, 9, 12]</td>\n",
       "      <td>[attribute definitions, catalogs, catalog stru...</td>\n",
       "      <td>[Attribute_Definitions, Catalogs, Catalog_Stru...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>166 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          column_names  \\\n",
       "0    [[-1, *], [0, perpetrator id], [0, people id],...   \n",
       "1    [[-1, *], [0, building], [0, room number], [0,...   \n",
       "2    [[-1, *], [0, id], [0, city], [0, country], [0...   \n",
       "3    [[-1, *], [0, institution id], [0, name], [0, ...   \n",
       "4    [[-1, *], [0, body builder id], [0, people id]...   \n",
       "..                                                 ...   \n",
       "161  [[-1, *], [0, employee ssn], [0, project numbe...   \n",
       "162  [[-1, *], [0, workshop id], [0, date], [0, ven...   \n",
       "163  [[-1, *], [0, item id], [0, title], [1, a id],...   \n",
       "164  [[-1, *], [0, party id], [0, party theme], [0,...   \n",
       "165  [[-1, *], [0, attribute id], [0, attribute nam...   \n",
       "\n",
       "                                 column_names_original  \\\n",
       "0    [[-1, *], [0, Perpetrator_ID], [0, People_ID],...   \n",
       "1    [[-1, *], [0, building], [0, room_number], [0,...   \n",
       "2    [[-1, *], [0, id], [0, City], [0, Country], [0...   \n",
       "3    [[-1, *], [0, instID], [0, name], [0, country]...   \n",
       "4    [[-1, *], [0, Body_Builder_ID], [0, People_ID]...   \n",
       "..                                                 ...   \n",
       "161  [[-1, *], [0, Essn], [0, Pno], [0, Hours], [1,...   \n",
       "162  [[-1, *], [0, Workshop_ID], [0, Date], [0, Ven...   \n",
       "163  [[-1, *], [0, i_id], [0, title], [1, a_id], [1...   \n",
       "164  [[-1, *], [0, Party_ID], [0, Party_Theme], [0,...   \n",
       "165  [[-1, *], [0, attribute_id], [0, attribute_nam...   \n",
       "\n",
       "                                          column_types            db_id  \\\n",
       "0    [text, number, number, text, number, text, tex...      perpetrator   \n",
       "1    [text, text, text, number, text, text, number,...        college_2   \n",
       "2    [text, number, text, text, text, text, text, n...   flight_company   \n",
       "3    [text, number, text, text, number, text, text,...           icfp_1   \n",
       "4    [text, number, number, number, number, number,...     body_builder   \n",
       "..                                                 ...              ...   \n",
       "161  [text, number, number, number, text, text, tex...        company_1   \n",
       "162  [text, number, text, text, text, number, numbe...   workshop_paper   \n",
       "163  [text, number, text, number, number, number, n...       epinions_1   \n",
       "164  [text, number, text, text, text, text, number,...       party_host   \n",
       "165  [text, number, text, text, number, text, text,...  product_catalog   \n",
       "\n",
       "                                          foreign_keys  \\\n",
       "0                                             [[2, 9]]   \n",
       "1    [[9, 4], [13, 4], [19, 1], [20, 2], [15, 7], [...   \n",
       "2                                   [[20, 7], [19, 1]]   \n",
       "3                           [[11, 7], [10, 1], [9, 4]]   \n",
       "4                                             [[2, 6]]   \n",
       "..                                                 ...   \n",
       "161                                                 []   \n",
       "162                                  [[10, 1], [9, 5]]   \n",
       "163                 [[5, 1], [4, 8], [11, 8], [10, 8]]   \n",
       "164                                 [[11, 1], [12, 7]]   \n",
       "165              [[10, 4], [13, 9], [27, 9], [26, 12]]   \n",
       "\n",
       "                                  primary_keys  \\\n",
       "0                                       [1, 9]   \n",
       "1    [1, 4, 7, 11, 15, 22, 27, 31, 37, 39, 45]   \n",
       "2                                   [1, 7, 13]   \n",
       "3                                 [1, 4, 7, 9]   \n",
       "4                                       [1, 6]   \n",
       "..                                         ...   \n",
       "161                     [1, 7, 15, 19, 22, 27]   \n",
       "162                                  [1, 5, 9]   \n",
       "163                                  [1, 3, 8]   \n",
       "164                                 [1, 7, 11]   \n",
       "165                              [1, 4, 9, 12]   \n",
       "\n",
       "                                           table_names  \\\n",
       "0                                [perpetrator, people]   \n",
       "1    [classroom, department, course, instructor, se...   \n",
       "2                   [airport, operate company, flight]   \n",
       "3     [institution, authors, papers, authorship count]   \n",
       "4                               [body builder, people]   \n",
       "..                                                 ...   \n",
       "161  [works on, employee, department, project, depe...   \n",
       "162                 [workshop, submission, acceptance]   \n",
       "163                    [item, review, useracct, trust]   \n",
       "164                          [party, host, party host]   \n",
       "165  [attribute definitions, catalogs, catalog stru...   \n",
       "\n",
       "                                  table_names_original  \n",
       "0                                [perpetrator, people]  \n",
       "1    [classroom, department, course, instructor, se...  \n",
       "2                   [airport, operate_company, flight]  \n",
       "3                  [Inst, Authors, Papers, Authorship]  \n",
       "4                               [body_builder, people]  \n",
       "..                                                 ...  \n",
       "161  [works_on, employee, department, project, depe...  \n",
       "162                 [workshop, submission, Acceptance]  \n",
       "163                    [item, review, useracct, trust]  \n",
       "164                          [party, host, party_host]  \n",
       "165  [Attribute_Definitions, Catalogs, Catalog_Stru...  \n",
       "\n",
       "[166 rows x 8 columns]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>db_id</th>\n",
       "      <th>query</th>\n",
       "      <th>question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>geo</td>\n",
       "      <td>SELECT MAX ( highest_elevation ) FROM highlow</td>\n",
       "      <td>what is the height of the highest point in the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>geo</td>\n",
       "      <td>SELECT COUNT ( city_name ) FROM city</td>\n",
       "      <td>how many cities are there in usa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cre_Doc_Tracking_DB</td>\n",
       "      <td>SELECT DISTINCT location_code FROM Document_lo...</td>\n",
       "      <td>What are the different location codes for docu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>program_share</td>\n",
       "      <td>SELECT name FROM program ORDER BY launch DESC ...</td>\n",
       "      <td>find the name of the program that was launched...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>college_2</td>\n",
       "      <td>SELECT name FROM instructor ORDER BY salary DE...</td>\n",
       "      <td>Give the name of the highest paid instructor.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7749</th>\n",
       "      <td>tvshow</td>\n",
       "      <td>SELECT T1.series_name FROM TV_Channel AS T1 JO...</td>\n",
       "      <td>What is the TV Channel of TV series with Episo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7750</th>\n",
       "      <td>restaurants</td>\n",
       "      <td>SELECT t2.house_number  ,  t1.name FROM LOCATI...</td>\n",
       "      <td>what is a good restaurant in the bay area ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7751</th>\n",
       "      <td>music_2</td>\n",
       "      <td>SELECT TYPE FROM vocals GROUP BY TYPE ORDER BY...</td>\n",
       "      <td>Which vocal type is the most frequently appear...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7752</th>\n",
       "      <td>scholar</td>\n",
       "      <td>SELECT DISTINCT t3.venueid FROM writes AS t2 J...</td>\n",
       "      <td>Where did li dong publish in 2016 ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7753</th>\n",
       "      <td>battle_death</td>\n",
       "      <td>SELECT T2.id ,  T2.name FROM death AS T1 JOIN ...</td>\n",
       "      <td>What is the ship id and name that caused most ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7754 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    db_id                                              query  \\\n",
       "0                     geo      SELECT MAX ( highest_elevation ) FROM highlow   \n",
       "1                     geo               SELECT COUNT ( city_name ) FROM city   \n",
       "2     cre_Doc_Tracking_DB  SELECT DISTINCT location_code FROM Document_lo...   \n",
       "3           program_share  SELECT name FROM program ORDER BY launch DESC ...   \n",
       "4               college_2  SELECT name FROM instructor ORDER BY salary DE...   \n",
       "...                   ...                                                ...   \n",
       "7749               tvshow  SELECT T1.series_name FROM TV_Channel AS T1 JO...   \n",
       "7750          restaurants  SELECT t2.house_number  ,  t1.name FROM LOCATI...   \n",
       "7751              music_2  SELECT TYPE FROM vocals GROUP BY TYPE ORDER BY...   \n",
       "7752              scholar  SELECT DISTINCT t3.venueid FROM writes AS t2 J...   \n",
       "7753         battle_death  SELECT T2.id ,  T2.name FROM death AS T1 JOIN ...   \n",
       "\n",
       "                                               question  \n",
       "0     what is the height of the highest point in the...  \n",
       "1                      how many cities are there in usa  \n",
       "2     What are the different location codes for docu...  \n",
       "3     find the name of the program that was launched...  \n",
       "4         Give the name of the highest paid instructor.  \n",
       "...                                                 ...  \n",
       "7749  What is the TV Channel of TV series with Episo...  \n",
       "7750        what is a good restaurant in the bay area ?  \n",
       "7751  Which vocal type is the most frequently appear...  \n",
       "7752                Where did li dong publish in 2016 ?  \n",
       "7753  What is the ship id and name that caused most ...  \n",
       "\n",
       "[7754 rows x 3 columns]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(\"../../data/sql_query.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %%\n",
    "train_file = \"../../data/Text-To-SQL-COL775/train.csv\"\n",
    "table_file = \"../../data/Text-To-SQL-COL775/tables.json\"\n",
    "val_file = \"../../data/Text-To-SQL-COL775/val.csv\"\n",
    "test_file = \"../../data/sql_query.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question = df_train.iloc[0]['question'] # question extracted\n",
    "# query = df_train.iloc[0]['query'] # query extracted\n",
    "# schema = df_train.iloc[0]['db_id'] # get database id\n",
    "# # Get the table and column names for the database schema\n",
    "# tables = df_tables[df_tables['db_id'] == schema]['table_names_original'].tolist() # entire table data for that db_id\n",
    "# columns = df_tables[df_tables['db_id'] == schema]['column_names_original'].tolist()\n",
    "# # Convert the table and column names to a list of strings\n",
    "# tables = [item for sublist in tables for item in sublist]\n",
    "# columns = [item for sublist in columns for item in sublist]\n",
    "# tables = [str(table) for table in tables]\n",
    "# columns = [str(column) for column in columns]\n",
    "# # Concatenate the table and column names to the question and query inputs\n",
    "# question += ' ' + ' '.join(tables) + ' ' + ' '.join(columns)\n",
    "# query += ' ' + ' '.join(tables) + ' ' + ' '.join(columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpiderDataset(data.Dataset):\n",
    "    def __init__(self, data_file, table_file):\n",
    "        self.data = pd.read_csv(data_file)\n",
    "        self.table_data = pd.read_json(table_file)\n",
    "        self.vocab = GloVe(name='6B', dim=100, unk_init=torch.Tensor.normal_)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        question = self.data.iloc[index]['question'] # question extracted\n",
    "        query = self.data.iloc[index]['query'] # query extracted\n",
    "        schema = self.data.iloc[index]['db_id'] # get database id\n",
    "        # Get the table and column names for the database schema\n",
    "        tables = self.table_data[self.table_data['db_id'] == schema]['table_names_original'].tolist() # entire table data for that db_id\n",
    "        columns = self.table_data[self.table_data['db_id'] == schema]['column_names_original'].tolist()\n",
    "        # Convert the table and column names to a list of strings\n",
    "        tables = [item for sublist in tables for item in sublist]\n",
    "        columns = [item for sublist in columns for item in sublist]\n",
    "        tables = [str(table) for table in tables]\n",
    "        columns = [str(column) for column in columns]\n",
    "        # Concatenate the table and column names to the question and query inputs\n",
    "        question += ' ' + ' '.join(tables) + ' ' + ' '.join(columns)\n",
    "        query += ' ' + ' '.join(tables) + ' ' + ' '.join(columns)\n",
    "\n",
    "        return question, query, schema\n",
    "    \n",
    "    def collate_fn(self, batch):\n",
    "        questions, queries, schemas = zip(*batch)\n",
    "        # Tokenize and convert to tensors\n",
    "        question_seq = [torch.tensor([self.vocab.stoi.get(token, self.vocab.stoi['unk']) for token in question.split() if token != '']) for question in questions]\n",
    "        query_seq = [torch.tensor([self.vocab.stoi.get(token, self.vocab.stoi['unk']) for token in query.split() if token != '']) for query in queries]\n",
    "        schema_seq = [torch.tensor([self.vocab.stoi.get(token, self.vocab.stoi['unk']) for token in schema.split() if token != '']) for schema in schemas]\n",
    "        # Pad sequences to the same length\n",
    "        question_seq = nn.utils.rnn.pad_sequence(question_seq, batch_first=True, padding_value=0)\n",
    "        query_seq = nn.utils.rnn.pad_sequence(query_seq, batch_first=True, padding_value=0)\n",
    "        schema_seq = nn.utils.rnn.pad_sequence(schema_seq, batch_first=True, padding_value=0)\n",
    "\n",
    "        return question_seq, query_seq, schema_seq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create instances of the SpiderDataset class for each set\n",
    "train_dataset = SpiderDataset(train_file, table_file)\n",
    "val_dataset = SpiderDataset(val_file, table_file)\n",
    "test_dataset = SpiderDataset(test_file, table_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"What are the different location codes for documents? Ref_Document_Types Ref_Calendar Ref_Locations Roles All_Documents Employees Document_Locations Documents_to_be_Destroyed [-1, '*'] [0, 'Document_Type_Code'] [0, 'Document_Type_Name'] [0, 'Document_Type_Description'] [1, 'Calendar_Date'] [1, 'Day_Number'] [2, 'Location_Code'] [2, 'Location_Name'] [2, 'Location_Description'] [3, 'Role_Code'] [3, 'Role_Name'] [3, 'Role_Description'] [4, 'Document_ID'] [4, 'Date_Stored'] [4, 'Document_Type_Code'] [4, 'Document_Name'] [4, 'Document_Description'] [4, 'Other_Details'] [5, 'Employee_ID'] [5, 'Role_Code'] [5, 'Employee_Name'] [5, 'Gender_MFU'] [5, 'Date_of_Birth'] [5, 'Other_Details'] [6, 'Document_ID'] [6, 'Location_Code'] [6, 'Date_in_Location_From'] [6, 'Date_in_Locaton_To'] [7, 'Document_ID'] [7, 'Destruction_Authorised_by_Employee_ID'] [7, 'Destroyed_by_Employee_ID'] [7, 'Planned_Destruction_Date'] [7, 'Actual_Destruction_Date'] [7, 'Other_Details']\",\n",
       " \"SELECT DISTINCT location_code FROM Document_locations Ref_Document_Types Ref_Calendar Ref_Locations Roles All_Documents Employees Document_Locations Documents_to_be_Destroyed [-1, '*'] [0, 'Document_Type_Code'] [0, 'Document_Type_Name'] [0, 'Document_Type_Description'] [1, 'Calendar_Date'] [1, 'Day_Number'] [2, 'Location_Code'] [2, 'Location_Name'] [2, 'Location_Description'] [3, 'Role_Code'] [3, 'Role_Name'] [3, 'Role_Description'] [4, 'Document_ID'] [4, 'Date_Stored'] [4, 'Document_Type_Code'] [4, 'Document_Name'] [4, 'Document_Description'] [4, 'Other_Details'] [5, 'Employee_ID'] [5, 'Role_Code'] [5, 'Employee_Name'] [5, 'Gender_MFU'] [5, 'Date_of_Birth'] [5, 'Other_Details'] [6, 'Document_ID'] [6, 'Location_Code'] [6, 'Date_in_Location_From'] [6, 'Date_in_Locaton_To'] [7, 'Document_ID'] [7, 'Destruction_Authorised_by_Employee_ID'] [7, 'Destroyed_by_Employee_ID'] [7, 'Planned_Destruction_Date'] [7, 'Actual_Destruction_Date'] [7, 'Other_Details']\",\n",
       " 'cre_Doc_Tracking_DB')"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn=train_dataset.collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, batch in enumerate(train_loader):\n",
    "    \n",
    "    qt, qy, sch = [tensor.to(device) for tensor in batch]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 84]), torch.Size([2, 92]), torch.Size([2, 1]))"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qt.shape, qy.shape, sch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0015,  0.0926,  0.0375,  ..., -0.0717,  0.0031, -0.0512],\n",
       "         [-0.0216,  0.0792,  0.0453,  ..., -0.0565,  0.0082, -0.0385],\n",
       "         [-0.0315,  0.0605,  0.0532,  ..., -0.0577,  0.0109, -0.0473],\n",
       "         ...,\n",
       "         [-0.0250,  0.0145,  0.0701,  ..., -0.0471,  0.0004, -0.0715],\n",
       "         [-0.0250,  0.0144,  0.0701,  ..., -0.0470,  0.0004, -0.0716],\n",
       "         [-0.0250,  0.0143,  0.0701,  ..., -0.0470,  0.0004, -0.0716]],\n",
       "\n",
       "        [[-0.0508,  0.0684,  0.0403,  ..., -0.0801,  0.0340, -0.0170],\n",
       "         [-0.0372,  0.0750,  0.0418,  ..., -0.0809,  0.0292, -0.0283],\n",
       "         [-0.0325,  0.0604,  0.0520,  ..., -0.0743,  0.0241, -0.0466],\n",
       "         ...,\n",
       "         [-0.0069,  0.0973,  0.0413,  ..., -0.0432, -0.0205, -0.0612],\n",
       "         [-0.0069,  0.0973,  0.0413,  ..., -0.0432, -0.0205, -0.0612],\n",
       "         [-0.0069,  0.0973,  0.0413,  ..., -0.0432, -0.0205, -0.0612]]],\n",
       "       device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(qt, sch )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Seq2SeqLSTM(nn.Module):\n",
    "#     def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers):\n",
    "#         super(Seq2SeqLSTM, self).__init__()\n",
    "#         self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "#         self.encoder = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, batch_first=True)\n",
    "#         self.decoder = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, batch_first=True)\n",
    "#         self.linear = nn.Linear(hidden_dim, vocab_size)\n",
    "        \n",
    "#     def forward(self, input_seq, target_seq, input_lengths):\n",
    "#         embedded_input = self.embedding(input_seq)\n",
    "#         packed_input = pack_padded_sequence(embedded_input, input_lengths, batch_first=True, enforce_sorted=False)\n",
    "#         packed_output, hidden = self.encoder(packed_input)\n",
    "#         encoder_output, _ = pad_packed_sequence(packed_output, batch_first=True)\n",
    "#         decoder_output, _ = self.decoder(embedded_input, hidden)\n",
    "#         output = self.linear(decoder_output)\n",
    "#         return output\n",
    "\n",
    "class Seq2SeqLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers):\n",
    "        super(Seq2SeqLSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.encoder = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, batch_first=True)\n",
    "        self.decoder = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, input_seq, target_seq):\n",
    "        embedded_input = self.embedding(input_seq)\n",
    "        encoded_seq, (hidden, cell) = self.encoder(embedded_input)\n",
    "        decoded_seq, _ = self.decoder(embedded_input, (hidden, cell))\n",
    "        output_seq = self.linear(decoded_seq)\n",
    "        return output_seq\n",
    "\n",
    "\n",
    "# class Seq2SeqLSTM(nn.Module):\n",
    "#     def __init__(self, input_size, hidden_size, output_size, num_layers=1, dropout=0.2):\n",
    "#         super(Seq2SeqLSTM, self).__init__()\n",
    "#         self.hidden_size = hidden_size\n",
    "#         self.num_layers = num_layers\n",
    "#         self.dropout = dropout\n",
    "#         self.encoder = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "#         self.decoder = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "#         self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         # x is of shape (batch_size, seq_length, input_size)\n",
    "#         # Set initial states\n",
    "\n",
    "#         h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "#         c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "#         # Forward propagate encoder\n",
    "#         encoder_output, (hidden, cell) = self.encoder(x, (h0, c0))\n",
    "#         # Forward propagate decoder\n",
    "#         decoder_output, _ = self.decoder(encoder_output, (hidden, cell))\n",
    "#         # Decode the hidden state of the last time step\n",
    "#         out = self.fc(decoder_output[:, -1, :])\n",
    "#         return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400000"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "vocab_size = len(train_dataset.vocab)   \n",
    "input_size = 100\n",
    "hidden_size = 256\n",
    "num_layers = 1\n",
    "num_epochs = 5\n",
    "learning_rate = 0.001\n",
    "batch_size = 32\n",
    "#dropout = 0.2\n",
    "output_size = vocab_size\n",
    "\n",
    "\n",
    "#model = Seq2SeqLSTM(input_size, hidden_size, output_size, num_layers, dropout).to(device)\n",
    "#model = Seq2SeqLSTM(input_size, hidden_size, output_size, num_layers).to(device)\n",
    "model = Seq2SeqLSTM(vocab_size, embedding_dim=100, hidden_dim=256, num_layers=2).to(device)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Create data loaders for each set using the SpiderDataset.collate_fn method\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=train_dataset.collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=val_dataset.collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=test_dataset.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/122 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 213]) torch.Size([64, 212]) torch.Size([64, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "forward() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[116], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m questions, queries, input_lengths \u001b[39m=\u001b[39m batch\n\u001b[1;32m      8\u001b[0m \u001b[39mprint\u001b[39m(questions\u001b[39m.\u001b[39mshape, queries\u001b[39m.\u001b[39mshape, input_lengths\u001b[39m.\u001b[39mshape)\n\u001b[0;32m----> 9\u001b[0m output \u001b[39m=\u001b[39m model(questions, queries[:,:\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m])\n\u001b[1;32m     10\u001b[0m target \u001b[39m=\u001b[39m queries[:,\u001b[39m1\u001b[39m:]\n\u001b[1;32m     11\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(output\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, vocab_size), target\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\n",
      "File \u001b[0;32m~/miniconda3/envs/utk_equiformer/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = 0.0\n",
    "    val_loss = 0.0\n",
    "    model.train()\n",
    "    for i, batch in enumerate(tqdm(train_loader)):\n",
    "        optimizer.zero_grad()\n",
    "        questions, queries, input_lengths = batch\n",
    "        print(questions.shape, queries.shape, input_lengths.shape)\n",
    "        output = model(questions, queries[:,:-1])\n",
    "        target = queries[:,1:]\n",
    "        loss = loss_fn(output.reshape(-1, vocab_size), target.reshape(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    train_loss /= len(train_loader)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(val_loader):\n",
    "            questions, queries, input_lengths = batch\n",
    "            output = model(questions, queries[:,:-1])\n",
    "            target = queries[:,1:]\n",
    "            loss = loss_fn(output.reshape(-1, vocab_size), target.reshape(-1))\n",
    "            val_loss += loss.item()\n",
    "        val_loss /= len(val_loader)\n",
    "    print(f\"Epoch {epoch+1}: Train Loss={train_loss:.4f}, Val Loss={val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() takes 2 positional arguments but 4 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[115], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model(questions, queries[:,:\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m], input_lengths)\n",
      "File \u001b[0;32m~/miniconda3/envs/utk_equiformer/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() takes 2 positional arguments but 4 were given"
     ]
    }
   ],
   "source": [
    "model(questions, queries[:,:-1], input_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[201534, 201534,      1,  ...,      0,      0,      0],\n",
       "        [201534,  13697, 201534,  ...,      0,      0,      0],\n",
       "        [201534, 201534,      1,  ...,      0,      0,      0],\n",
       "        ...,\n",
       "        [201534, 201534,      1,  ...,      0,      0,      0],\n",
       "        [201534, 201534, 201534,  ...,      0,      0,      0],\n",
       "        [201534, 201534, 201534,  ...,      0,      0,      0]])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries[:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[201534,      1, 201534,  ...,      0,      0,      0],\n",
       "        [ 13697, 201534, 201534,  ...,      0,      0,      0],\n",
       "        [201534,      1, 201534,  ...,      0,      0,      0],\n",
       "        ...,\n",
       "        [201534,      1, 201534,  ...,      0,      0,      0],\n",
       "        [201534, 201534,  33537,  ...,      0,      0,      0],\n",
       "        [201534, 201534, 201534,  ...,      0,      0,      0]])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 220])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3440, 400000]), torch.Size([3392]))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.reshape(-1, vocab_size).shape, target.reshape(-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'How' is present in the vocabulary\n"
     ]
    }
   ],
   "source": [
    "from torchtext.vocab import GloVe\n",
    "\n",
    "# Load the GloVe word embeddings\n",
    "#vocab = GloVe(name='6B', dim=100)\n",
    "\n",
    "# Check if the 'How' token is present in the vocabulary\n",
    "if 'How' in vocab.stoi:\n",
    "    print(\"'How' is present in the vocabulary\")\n",
    "else:\n",
    "    print(\"'How' is not present in the vocabulary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'How' has been added to the vocabulary\n"
     ]
    }
   ],
   "source": [
    "from torchtext.vocab import GloVe\n",
    "\n",
    "# Load the GloVe word embeddings\n",
    "vocab = GloVe(name='6B', dim=100)\n",
    "\n",
    "# Add the 'How' token to the vocabulary with a new embedding vector\n",
    "new_token = 'How'\n",
    "new_embedding = torch.randn(1, 100)  # Generate a new random embedding vector for the token\n",
    "vocab.stoi[new_token] = len(vocab.stoi)  # Add the new token to the vocabulary\n",
    "vocab.vectors = torch.cat((vocab.vectors, new_embedding), dim=0)  # Add the new embedding vector to the GloVe embeddings tensor\n",
    "\n",
    "# Check if the 'How' token is now present in the vocabulary\n",
    "if 'How' in vocab.stoi:\n",
    "    print(\"'How' has been added to the vocabulary\")\n",
    "else:\n",
    "    print(\"Error: 'How' was not added to the vocabulary\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
